Prep:
write the definition of the Mandelbrot set
draw the abstract value lattice
draw the flow analysis example

<mandel demo> (with timings of Mflops per second)
- This is the Mandelbrot set
  - c is colored black if iterating z = z^2 + c
    does not escape
  - in this case, in 400 steps
  - antialiased for your viewing pleasure
- Stupid algorithm to flagellate the computer
  - 10 flops per step x 400 steps per point x 4
    points per pixel

What do you think the inner loop of that program
looks like?

You might be forgiven for thinking it looks like
this:
<show asm.js source>
<discuss out extreme non-modularity>:
- Complex number operations
- Iteration
- Definition of what we're doing

But the source code I actually wrote for that
looks like this:
<slide with DVL sources>
- Look how modular this code is
- But if you just tried to compile this on a
  normal system, it would be dog slow (unless you
  got lucky because this is such a tiny example)

Modularity is inherently expensive because it's
the ability to reuse the same idea in many
different ways and in many different places.
- In this case, iterate a different function; or
  use the step function in a different way
- Its price is generic linkage between the "idea"
  and the "places"
- The linkage is its own cost, and it impedes
  optimizations that span parts of the "idea" and
  the "place"

What's the trouble here?

Can't to anything to iterate from its definition
because don't know f.

Can't do anything to the interface of (step c)
because don't know all the places it is called.

But by tracing a little of the control+data flow
of this program, we can discover that f is (step
c), and that (step c) gets called only at (f x)

- This will enable all sorts of boundary-crossing
  optimizations down the line (like getting rid
  of the allocations and references)

I suspect that this is what humans do when they
notice their compilers are not Sufficiently
Clever.

So, how to gather useful information from flow?

I should say now that this is a pure-functional
language intended for compute-intensive inner
loops only.
- The mechanism I will show you does not do well
  on "large complex programs".
- Scientific computing kernels where the floating
  point is what's interesting are a good fit.
- Solve the scaffolding at compile time, do the
  arithmetic at runtime.

One slide for the cognoscenti

Idea for the rest of us:

Interpret the program abstractly, indirecting
through a table of possible abstract states.
- An "abstract value" represents a set of
  possible actual values; these sets form a
  lattice
<draw the abstract value domain>
<flip back to dvl code>
- For a functional language, the program state is
  expression-environment or operator-operand, and
  we need the (abstract) return value from there.
- Do one step of interpretation at a time and
  indirect through the table for all the
  unknowns
- Expand the table when you need more stuff
<walk through iterate example>
Now that you've got this information, what do you
do with it?

One more slide for the cognoscenti

Idea for the rest of us:

For every operator-operand state, generate one
(toplevel, closure-converted) procedure in the
intermediate language.

You know the operator-operand shapes of all calls
it makes, so emit static calls to those
procedures.

This accomplishes constant propagation and
type-directed specialization (for a very
fine-grained and informative system of types).

<show, walk through>

op-23 is a static call, and already knows what
the control flow is going to be.

The only things flowing here are (union-free!)
data structures.

Since I know all the places op-23 is called, can
freely change its interface to represent complex
numbers as pairs of real variables rather than
pointers to objects (this is scalar replacement
of aggregates)
- those variables can then live in registers
  instead of stack or heap

Are we fast yet?  No, but the rest of the way is
textbook.

If there's time, show the result of fully
optimized FOL, right before translation to
asm.js.
- This intermediate language emerged before I had
  ever heard of asm.js
- And then it took about a week to write the
  translator
- Also have backends to MIT Scheme, SBCL, GHC

Functional but also support automatic
differentiation (but that's another talk)

Not too complex -- modular constructs are
scaffolding, which needs to be solvable at
compile time.

Caveats:
- Analysis may not terminate, and may use huge
  amounts of RAM even when it does

- Depends upon programmer annotations to
  introduce imprecision, otherwise will execute
  the program slowly at compile time

- Research prototype quality software in general
  (type error in the program gives prompt
  debugging the compiler)

- The hope is to get it to be better than doing
  the same specializations by hand
By the way, I suspect that flowing information
from uses to definitions is what people actually
do when they come up with optimizations that seem
obvious but their compiler is not Sufficiently
Clever to find.

-------------------------------------------------
Uncertainties about the audience:

- How much do they know about language
  implementation/compilers

- How much do they know about partial
  evaluation/flow analysis

Partial evaluation, supercompilation, and the
flow analysis in DVL all specialize to all
possible contexts at compile time.  JITs
specialize to observed contexts at runtime.

