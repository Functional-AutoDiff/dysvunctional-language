                FOL as an Intermediate Representation

FOL is not really a programming language; it's an intermediate
representation for the insides of a compiler.  It happens to be stored
as a cons-tree with symbols in it, and these symbols happen to have
the traditional Scheme meanings.  FOL source therefore happens to
print out like Scheme code, which makes it much easier to read; and it
also happens to ship with a direct interpreter, which gives it a
semantics that can be used to verify correctness of transformations.
But how is FOL source different from any other intermediate
representation?

FOL occupies a slot in the architecture of VL and DVL that is parallel
to the place where control flow graphs (CFGs) usually sit in other
compilers.  Like a CFG, FOL source statically exposes as much
information about the dynamic control flow of the program it
represents as it possibly can.  Like a CFG, FOL is therefore amenable
to dataflow-based optimizations, like elimination of dead variables
and common subexpressions.

Masking that fundamental similarity are a bunch of superficial
differences; there is, however, also one very substantial difference.
I compare here to the CFGs in Appel [1], since those are the ones I
know.  Appel discusses three variants: plain vanilla CFGs (Chapters 17
and 18), Static Single Assignment Form (Chapter 19), and the otherwise
nameless "functional representation" (Section 19.7).  Since they are
all equivalent [*], I will compare to CFGs, and mention the others at
need.

- FOL permits expressions, whereas CFGs permit only instructions.
  Expressions have substructure, and the substructures have implicit
  returns, whereas CFG instructions just read stored data for their
  arguments.  Sufficiently aggressive conversion to A-normal form
  erases this difference.

- All variables in a CFG are in scope in the entire graph, whereas FOL
  bindings have limmited scope.  This is not an essential difference;
  and bindings in the functional representation have limited scope
  too.

- CFGs allow assignments to their variables, whereas FOL does not.
  This is not an essential difference; conversion to SSA mitigates it
  and conversion to the functional representation eliminates it
  entirely.

- The model of un-analyzed memory is different: All three of Appel's
  intermediate representations have primitives that read or write a
  global store at computed indices, whereas FOL has primitives that
  construct and access immutable objects on a garbage-collected heap.
  I don't think this is an essential difference; just that the
  understanding of the behavior of control is done with respect to a
  different level of abstraction of the memory system.

- The important difference is that Appel and FOL make a different
  subdivision of the entire program into "procedures".  Appel does not
  discuss how to represent a whole program; presumably a map from
  procedure names to the CFGs for those procedures would do.  The
  toplevel (begin ...) form in the FOL grammar effectively constitutes
  such a map, whose entries are the (define (foo ...) ...) forms.

  - An Appel procedure is a CFG; as such it contains labels and
    branches, so it may run loops, but does not have access to a stack
    (except for calling other procedures, but those are intentionally
    outside the scope of the analyses he discusses).

  - A FOL procedure cannot even have loops.  Viewed as a CFG it is
    additionally restricted to be acyclic.

- These subdivisions have different effects on ease of analysis:

  If we are no longer analyzing any data that flows through the
  backing memory (Appel's store or FOL's data structure slots), then
  CFGs are restricted to unbounded-time but bounded-space
  computations.  If we further do not work too hard on discerning the
  different data values that may grace a CFG's variables, they become
  finite-state (except for the non-determinism of multiple possible
  continuations of a branch), and can therefore be analyzed by
  iterating some dataflow crunch to convergence.

  FOL procedures, being more restricted, are easier to analyze: since
  the control flow graph is acyclic, you can do a data flow analysis
  on it in one sweep, provided you sweep in the topological sort order
  (or in reverse).  This is both simpler to code and faster to
  execute.

  In fact, the representation of FOL procedures as Scheme source code
  has the effect of storing them in a tree whose in-order traversal is
  a topological sort traversal of the control flow graph they
  represent.

  FOL procedures are not, however, Appel's basic blocks: those are CFG
  subgraphs all but one of whose vertices has out-degree one and all
  but (a different) one of whose vertices has in-degree one.  Basic
  blocks are even easier to analyze, because you don't have to worry
  about branch-points or join-points.

- These subdivisions also have different effects on how whole programs
  are broken up:

  A CFG can represent a larger class of computations (bounded space
  but unbounded time) directly, so a whole program can be broken up
  into fewer CFG procedures; and less information is lost by
  forgetting information across procedure boundaries.

  Conversely, a FOL procedure can only represent a smaller class of
  computations (bounded space and time), so a FOL program may need to
  have more procedures, and lose more information across procedure
  boundaries.  In particular, FOL has no mechanism of allowing a
  single variable to remain in scope over a loop (indeed, all FOL
  variables are in scope over a bounded number of dynamic operations),
  so FOL cannot express the loop optimizations that are the subject of
  Appel's Chapter 18.

- FOL could be extended to match the expressive power of Appel's CFGs
  by adding a construct for locally defining (potentially
  mutually-recursive) functions that (may not be passed around as data
  and) may be called only in tail position.  The names of these
  functions would then behave like the labels of a standard CFG.  In
  fact, I believe this is exactly what Appel had in mind when he
  defined the "functional representation" in Section 19.7.

  The restriction to call these local functions only in tail position
  would have the effect that anything that looks like a return either
  flies into a statically known let binding, or flies out of the
  entire FOL-procedure; in one case the control transfer from a return
  is known, and in the other it is not modeled by an intraprocedural
  analysis.

- Inter-FOL-procedural analyses share some elements in common with
  intra-CFG-analyses, namely that they admit control cycles, and
  therefore must iterate some dataflow crunch to convergence.  On the
  other hand, there is more hair, for two reasons: The implicit stack
  means that you may be modeling an unbounded number of memory
  locations; and you may also wish to model the fact that a procedure
  will return to its caller.  The latter means that the call graph of
  the procedures is no longer the approximation you want for the
  control flow of a program implemented with those procedures.
  Finally, if you are not forgetting information across procedure call
  boundaries, then you must forget it somewhere else.  Presumably the
  same sort of hair arises for the inter-CFG-procedural analyses that
  are outside the scope of [1].

The fact that FOL as a representation is restricted, relative to CFGs,
is acceptable.  FOL's purpose is not to be an excellent compiler in
itself; it is just to clean up the messes made by the VL and DVL code
generators, so that the output can be read by a person, and reasonably
handed off to an external compiler.  The latter task is actually
nontrivially similar to the former, because end-to-end compilers tend
to have implicit assumptions about the "reasonableness" of their input
programs, and don't take care to run fast or do a good job with
"unreasonable" programs.

In fact, the historical inspiration for the analogue of FOL that
appears in Stalingrad was that gcc ran out of memory on programs that
demanded the inlining of thousands of tiny procedures; and the
historical inspiration for FOL itself, besides wanting to read the
output of VL, was that the MIT Scheme compiler choked on a procedure
that had 700 formal parameters (almost all of which were passing, to
recursive calls of the same procedure, pointers to the same object,
that it never even read.  They were all removable either by
interprocedural dead variable elimination or by (as-yet-unimplemented)
interprocedural alias elimination).  Both cases were instances of a
program emitting a valid program that any sane compiler writer would
(correctly!) assume no sane programmer would write.


[*] I assume that the local functions whose definitions Appel permits
in the "functional representation" are restricted to be called only in
tail position.  He did not actually say this; but I will assume it,
because without that restriction, the "functional representation" is
actually much more powerful than CFGs, and requires the full hair of
interprocedural analyses even for the interiors of its "procedures".

I also assume that those same local functions are second-class, and
may only be called by the names they are given.  Then they are reduced
to behaving just like (scoped) CFG labels; without this restriction,
one would need to do dataflow on the function objects to determine
control flow from call sites that called a function stored in a data
variable.


[1] Appel, Andrew W.  "Modern Compiler Implementation in ML".
Cambridge University Press, 2004.
