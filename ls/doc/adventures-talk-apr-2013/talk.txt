A great deal of this class has been about
modularity and flexibility.  I want to spend a
lecture talking about a way to avoid its costs.

Can get SBCL to within 2-3x of Fortran
- but only by writing your program in Lisptran
  (if with nice macros)

Fortran is fast simply because it doesn't give
you any modularity constructs that would confuse
the compilers.

Lisp compilers give comparable performance, but
only without those same modularity constructs.

There is another way; it has its own price.
What's the trouble with modularity?  Can't
optimize across module boundaries.
- (map f lst): don't know f, don't know where the
  lst came from
- modularity means map can be used in many
  places; can't improve the one global map
- small example: mandelbrot
  - Want to write (iterate (real 100) (step c) 0)
  - Want to get integrated arithmetic <show>
    no consing, no stack, just arithmetic
- bigger example: ODE simulation <show source,
  pages of crap output>

Local optimizations are mostly useless, because
(good) programmers tend to write fairly optimal
code within modularity boundaries (e.g.,
functions).
The only game in town for fixing this is copying
and specialization in some form.

Inlining: make copies of revelant functions that
are used only here, then do local optimizations.
- e.g., map-over-iota

Big problem: intermediate expression bulge.  You
have to copy (which costs) before seeing
optimizations (benefits).  This tends to get
combinatorially large.

Partial evaluation, supercompilation are all
attempts to organize interleaving the copying
with the reduction.

I will show you another way to organize copying
and specialization, from the perspective of flow
analysis.
Fundamentally, any kind of compiler has to have
an interpreter in it.

This interpreter has a problem: it has to go both
ways on IF.

To keep from running forever, have to stop
somewhere.

Can stop at module boundaries, but that defeats
today's purpose.

Other way: keep some representation of what's
been seen, stop if you see the "same thing"
again.

So the analysis structure is a kind of cache for
loop detection.
In this program, an analysis looks like this:
<draw bindings on the board>
- expression-environment -> value
- operator-argument -> value
- abstract values

A binding here is two things:
- An assertion that this value is the smallest
  cover of things this expression is known to
  return in this environment
  - Aside about purity
- A desire to find more possible things

We are going from the bottom up in order to find
the tightest solution
- Avoid temptation: Start precise, become sound

<draw lattice of abstract values>
- bottom, constants, abstract-boolean,
  abstract-real, (), cons, environments, closures
  with all possible bodies, top
- lattice simple; only good for some programs
Walk through example of factorial (on
abstract-real)
- mention what would happen if we went from the
  top down

<Show> interpreters, expand and "refine"
- On hand: driver loop, analysis data structure

IF is done as a primitive procedure that takes
closures
- Show IF?
On code generation:

Can generate first-order code directly from the
analysis.

Every apply binding (with unsolved return)
becomes a new procedure
- Already closure-converted
- IF requires some care, as usual

Result will have tons of small functions; not
efficient yet, but now amenable to standard
techniques since all call sites now known.
So that's about it.

References, credits

(super stands for "supervised", not "superior")
