                FOL as an Intermediate Representation

FOL is not really a programming language; it's an intermediate
representation for the insides of a compiler.  It happens to print out
like Scheme code, which makes it much easier to read; and it happens
to ship with a direct interpreter, which gives it a semantics that can
be used to verify correctness of transformations; but how else is it
different from any other intermediate representation?

FOL occupies a slot in the architecture of VL and DVL that is parallel
to the place where control flow graphs (CFGs) usually sit in other
compilers.  Like a CFG, FOL source statically exposes as much
information about the dynamic control flow of the program it
represents as it possibly can.  Like a CFG, FOL is therefore amenable
to flow-based optimizations, like elimination of dead variables and
common subexpressions.

Given that fundamental similarity, however, there are a number of
substantial differences.  I compare here to the representations in
Appel [1], since those are the ones I know.  In particular, I compare
to Control Flow Graphs (Chapters 17 and 18), Static Single Assignment
Form (Chapter 19), and the otherwise nameless Functional
Representation (19.7).  CFGs and SSA are equivalent, so I will only
remark on them separately at need.

- FOL is oriented around expressions, whereas CFGs are oriented around
  instructions.  Expressions have substructure, and the substructures
  have implicit returns, whereas CFG instructions just read stored
  data for their arguments.  Sufficiently aggressive conversion to
  A-normal form erases this difference.

- All variables in a CFG are in scope in the entire graph, whereas FOL
  bindings have scope.  This is not an essential difference.

- The CFGs from [1] allow assignments to their variables, whereas FOL
  does not.  This is not an essential difference; conversion to SSA
  mitigates it and conversion to the functional representation eliminates
  it entirely.

- The model of un-analyzed memory is different: All three of Appel's
  intermediate representations have primitives that read or write a
  global store at computed indexes, whereas FOL has primitives that
  construct and access immutable objects on a garbage-collected heap.
  I don't think this is an essential difference; just that the
  understanding of the behavior of control is done with respect to a
  different level of abstraction of the memory system.

- Appel and FOL make a different subdivision of the entire program
  into "procedures".  Appel does not discuss how to represent a whole
  program; presumably a map from procedure names to the CFGs for those
  procedures would do.

  - An Appel procedure is a CFG; as such it contains labels and
    branches, so it may run loops, but does not have access to a stack
    (except for calling other procedures, but those are intentionally
    outside the scope of the analyses he discusses).

  - A FOL procedure cannot have loops.  Viewed as a CFG it is
    additionally restricted to be acyclic.

- These subdivisions have different effects on ease of analysis:

  If we are no longer analyzing any data that flows through the
  backing memory (Appel's store or FOL's data structure slots), then
  CFGs are restricted to unbounded-time but bounded-space
  computations.  If we further do not work too hard on discerning the
  different data values that may grace a CFG's variables, they become
  finite-state (except for the non-determinism of multiple possible
  continuations of a branch), and can therefore be analyzed by
  iterating some dataflow crunch to convergence.

  FOL procedures, being more restricted, are easier to analyze: since
  the control flow graph is acyclic, you can do a data flow analysis
  on it in one sweep, provided you sweep in the topological sort order
  (or in reverse).

  In fact, representing FOL procedures as Scheme source code has the
  effect of storing them in a tree whose in-order traversal is a
  topological sort traversal of the control flow graph they represent.

  FOL procedures are not, however, Appel's basic blocks: those are CFG
  subgraphs all but one of whose vertices has out-degree one and all
  but (a different) one of whose vertices has in-degree one.  Basic
  blocks are even easier to analyze, because you don't have to worry
  about branch-points or join-points.

- These subdivisions also have different effects on how whole programs
  are broken up:

  A CFG can represent a larger class of computations (bounded space
  but unbounded time) directly, so a whole program can be broken up
  into fewer CFG procedures; and less information is lost by
  forgetting information across procedure boundaries.

  Conversely, a FOL procedure can only represent a smaller class of
  computations (bounded space and time), so a FOL program may need to
  have more procedures, and lose more information across procedure
  boundaries.  In particular, FOL has no mechanism of allowing a
  single variable to remain in scope over a loop (indeed, all FOL
  variables are in scope over a bounded number of dynamic operations),
  so FOL cannot express the loop optimizations that are the subject of
  Appel's Chapter 18.

- FOL could be extended to match the expressive power of Appel's CFGs
  by adding a construct for locally defining (potentially
  mutually-recursive) functions that (may not be passed around as data
  and) may be called only in tail position.  The names of these
  functions would then behave like the labels of a standard CFG.  In
  fact, I believe this is exactly what Appel had in mind when he
  defined the "functional representation" in section 19.7 of [1],
  except that he didn't say that the local functions may be called
  only in tail position.

  The restriction to call these local functions only in tail position
  would have the effect that anything that looks like a return either
  flies into a statically known let binding, or flies out of the
  entire FOL-procedure; in one case the control transfer from a return
  is known, and in the other it is not modeled by an intraprocedural
  analysis.


[1] Appel, Andrew W.  "Modern Compiler Implementation in ML".
Cambridge University Press, 2004.

----------------------------------------------------------------------

The thing that's unique about interprocedural stuff is that control
flows out of a procedure call into a place that's determined by the
place where it flowed in, and we care about tracing the correspondence
(whether it's implemented by watching the control flow explicitly or
by doing some data flow on some token (or continuation function) that
says where to go is immaterial).

----------------------------------------------------------------------

FOL intraprocedural: Dataflows on an acyclic control flow graph.  This
implies a bounded execution time and therefore a finite number of
modeled memory bits.  These are easy to do by following a topological
sort of the control flow graph (FOL source happens to store these
graphs in topologically sorted form).  Break the full program into
pieces with acyclic control (FOL procedures) and don't model flows of
control or data across the introduced boundaries.

Appel intraprocedural: Dataflows with a finite number of modeled bits
of data but an arbitrary control flow graph.  Break the whole program
into pieces (procedures) that each only use a finite number of modeled
bits of memory (temporaries), and don't model flows of control or data
across the boundaries of these pieces.

Interprocedural: Dataflows with unbounded modeled locations (because
of the procedure stack) and an arbitrary control flow graph.  Since
this is Turing complete, you can't ever get this "right".  You have to
forget somewhere.

The latter but also tracking context: control will flow out of a
procedure call into a place that is determined by where it flowed into
that procedure call.
- Also, if control flowed from here, then I know this about the data
  that flowed in, which I don't know in general.
- I think you can go quite wild wrt the fineness of the context you
  track.

----------------------------------------------------------------------

The art of analyzing programs is the art of forgetting judiciously.
If you forget nothing at all, your "analysis" will consist exactly of
running the entire program and writing down the answer.  In this case,
you will have "accelerated" the program a great deal, but the analysis
will be very slow (namely the runtime of the program) and its range of
validity will be that one run of that one program, so you will not be
able to reuse it.  If you forget everything, your analysis will
consist of doing nothing, will be very fast and very broadly
applicable, but will permit no improvement to the program you were
analyzing.

Different optimizing analyses are characterized by what they model and
what they forget, and different choices affect the speed of the
analysis, its reusability in different situations, and the amount of
improvement that doing it offers.  These three things are not traded
off immaculately; there are sweet spots in the design space that make
some sets of these choices definitely better than others for large and
predictable classes of programs.  In fact, the very reason we program
at all is that programs can cause computers to do "the same" things
many times, so that describing the task once to a computer is easier
than performing it that many times oneself.

A compiler analysis that forgets the differences between different
instances of such a task but remembers the characteristics that make
this task different from other tasks could execute in time
proportional to the complexity of the task, but yield total
performance improvements proportional to the number of times the task
is performed; if the latter multiple is enough larger than the former,
the analysis is worthwhile.

The purpose of this essay is to characterize the things the FOL
optimizers forget and remember, in contrast with the corresponding
choices made by other compiler organizations; and to discuss the
consequences these choices have on the structure of the FOL
implementation, the speed with which it executes, and the sorts of
optimizations it can and cannot perform.

Every ahead-of-time compiler commits, by its very nature, to at least
one form of forgetting: it must "forget", because it perforce does not
know, the data that the compilee program will operate on at runtime.
Anything that will be read off of disk files or network servers or
command terminals must necessarily be unknown to the compiler.  This
means that ahead-of-time compilation can at most be as slow as the
complexity (in whatever sense is appropriate) of the program being
compiled, must be applicable to any possible runtime data that the
program might be exposed to, and ought to yield optimization fruit if
the program experiences a sufficiently large total runtime.  Choices
made within the design space of ahead-of-time compilers are tradeoffs
within those bounds.

This enforced separation between what is already known and what is yet
to be known leads the mind naturally to the idea of the separation
between "program" and "data".  The "program" is the essentially fixed
thing we are thinking about how to improve, and the "data" is the
stuff that it will operate on in the future, to all possible values of
which we must be applicable.  There are other social and historical
forces that tend to make programs have one kind of structure and data
another; there are phenomena that blur those distinctions, for Lisp's
storied EVAL takes some "data" and causes it to become "program".
Indeed, if one is thinking about the compiler, the compiler itself is
the "program" and the compilee is the "data"; and if the compiler is
self-hosting, then the compiler can also play the role of "data".

Nevertheless, there is a natural separation between "program" and
"data".  The question of which individual atom of a program will be
executed when is called "control flow", and the question of what
journey some datum will take is called "data flow".  Even in the
absence of EVAL, in a higher-order program the control flow is
intertwined with the data flow, because a call site can invoke a
function that flows in through a variable.  In the beginning they
must, therefore, be analyzed together.  This is the job of VL/DVL, and
its peers like CFA (k- or otherwise) [Shivers].

FOL enters the scene after the integrated flow analysis.  The
essential scope-limiting assumption in FOL and its peers is that the
effect of the data flow on the control flow has been studied as far as
it will be, and FOL will study it no more.  Places of course remain
where the data must affect the control, (for example, IF nodes), but
by the time one reaches FOL it is considered good enough to assume
that an IF can go either way, and stop trying to predict which way it
will go.

Once this assumption is made, there remains some set of paths that one
deems it possible that the running program will take --- sequences of
atomic program elements that it may execute.  Trying to write down the
complete list thereof is both futile and brittle, so it must, of
necessity, be approximated.  A natural approximation is a static
Control Flow Graph (CFG) --- a directed graph whose nodes are atomic
program operations and whose edges are possible control transfers
between them.  The set of all the possible paths through such a graph
is one possible approximation to the set of execution paths that will
occur in a running program.

In programs in the wild, the actual control paths in fact have more
structure than a single, history-free CFG will represent.  There are
procedures, and call sites, and the procedures return to their callers
when done; whereas a CFG node for the end of a procedure must of
necessity list all the places where that procedure is called as
possible targets of outward transfers.  In practice, which way control
will go on exit depends on how it entered, but a direct CFG cannot
capture this.  There are several ways to approach this difficulty: one
can introduce additional data tokens that say where to transfer
control next, in which case one is back to data flow affecting control
flow; one can replicate subgraphs of the control flow graph depending
on context, in which case one must somehow decide when to stop, lest
one's CFG grow beyond all bound; one can somehow represent more than
just the graph structure, with the same caveat; or one can just forget
about it, and get coarser analyses.  The choice of whether or not to
model this behavior distinguishes intraprocedural analyses from
interprocedural ones.

If one assumes that one will not model the relationship between where
a procedure was called from and where it will return, the finest-grain
representation for modeling what's left is equivalent to the control
flow graphs taught by Appel [1].  That same book also discusses Static
Single Assignment Form, which is just a normalized form for a control
flow graph, so I will not discuss the distinction.

----------------------------------------------------------------------

Just as one has choices to make about what aspects of control flow one
will model, one also has choices about what aspects of data flow one
will model.






- Memory model

- One must still cut the program (which is how one can lose context
  and let a function that is constant somewhere else become unknown
  here even in the absence of EVAL)
