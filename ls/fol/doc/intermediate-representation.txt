                FOL as an Intermediate Representation

FOL is not really a programming language; it's an intermediate
representation for the insides of a compiler.  It happens to print out
like Scheme code, which makes it much easier to read; and it happens
to ship with a direct interpreter, which gives it a semantics that can
be used to verify correctness of transformations; but how else is it
different from any other intermediate representation?

FOL occupies a slot in the architecture of VL and DVL that is parallel
to the place where control flow graphs (CFGs) usually sit in other
compilers.  Like a CFG, FOL source statically exposes as much
information about the dynamic control flow of the program it
represents as it possibly can.  Like a CFG, FOL is therefore amenable
to flow-based optimizations, like elimination of dead variables and
common subexpressions.

Given that fundamental similarity, however, there are a number of
substantial differences.  I compare here to the CFGs in [1], since
those are the ones I know.

- FOL is oriented around expressions, whereas CFGs are oriented around
  instructions.  Expressions have substructure, and the substructures
  have implicit returns, whereas CFG instructions just read stored
  data for their arguments.  Sufficiently aggressive conversion to
  A-normal form erases this difference.

- A complete FOL program is "bigger" than a CFG from [1], because
  those CFGs are all intraprocedural.  FOL procedures are more
  involved than CFG labels because they accept arguments which are
  kept on an implicit stack; the procedure call conventions are not
  represented in FOL.

- A FOL procedure is "smaller" than a CFG from [1] because those CFGs
  contain labels and jumps, and can express (constant-space) loops
  directly in the CFG, whereas a FOL procedure execution always takes
  a bounded number of instructions (except for the effects of calling
  other procedures).

- A FOL procedure is "bigger" than a basic block from [1] because a
  FOL procedure may contain IFs.  So control flow within a FOL
  procedure is not entirely linear, but may contain branches.  Those
  branches, however, must always converge (except, again, for the
  effect for calling other procedures).

- All variables in a CFG are in scope in the entire graph, whereas FOL
  bindings have scope.  This is not an essential difference.

- The CFGs from [1] allow assignments to their variables, whereas FOL
  does not.  This is not an essential difference.

- Except for reading and writing the store, and calls to procedures
  outside the CFG, any CFG from [1] can be executed in constant space
  (but not necessarily constant time).  In contrast, any FOL procedure
  can be executed in constant space and time (except for calls to
  other procedures).

- The CFGs in [1] have access to a read-write, indexable store (which
  is shared with procedures external to the CFG).  FOL procedures do
  not; instead, they may allocate objects in a memory-managed heap,
  pass them to each other, and access their slots.  I don't think this
  is an essential difference; just that the understanding of the
  behavior of control is done with respect to a different level of
  abstraction of the memory system.

- FOL represents constant-space loops implicitly, via tail recursion
  across FOL procedure boundaries.  CFGs represent constant-space
  loops explicitly, by encoding them with branches inside a single
  CFG.  Therefore, restricting an analysis to intraprocedural in FOL
  is a stronger restriction (because it moves loops out of the
  purview), and does more to make the analysis easier to write.

- FOL procedures are amenable to cascading intra-FOL-procedural
  analysis-and-transformation passes.  Traverse the structure of the
  source recursively, carrying down any information you may need from
  the context.  Return the rewritten expression and any information
  about it that you need.  At a LET, analyze either the
  binding-expression or the body first, depending on whether you going
  in the same or the reverse direction as execution will,
  respectively.  At an IF, recur on the predicate and both branches
  and then merge.  CFG basic blocks are also amenable to such passes,
  but are smaller, so the passes are less useful.

- Intra-CFG-procedural analyses require actual graph dataflows,
  because they can express complicated loops.  In this,
  inter-FOL-procedural analyses are similar.  I suspect the latter are
  more complex, however, because of the implicit stack across
  procedure boundaries, and the consequence that any loops do not run
  in constant space.  I have never seen any descriptions of
  inter-CFG-procedural analyses, so I don't know what interesting
  properties they have in common with either intra-CFG-procedural
  analyses or inter-FOL-procedural analyses.

- The FOL representation has the weakness, relative to CFGs, that
  there is no way for a variable to be in scope over a time-unbounded
  extent.  FOL therefore does not capture loop-invariant data, and
  cannot express loop hoisting transformations.  On the other hand, my
  understanding is that since CFGs can already directly express the
  most common bounded-space computations, it is typical for CFG
  compilers to drop tail recursion optimization of CFG procedures (for
  better or for worse).

TODO Reread Appel Chapters 15, 18, and 19 with an eye to improving
FOL.  Remember the equivalence with the form described in 19.7.  With
something analagous to internal definitions, I should be able to
express (some) constant-space loops in FOL, and do all manner of loop
optimizations.  Or I can leave all that to GCC.


[1] Appel, Andrew W.  "Modern Compiler Implementation in ML".
Cambridge University Press, 2004.  Chapter 17.


Appel's book talks about CFGs, SSA form, and "a functional
representation".  If you restrict the latter to tail-calls only and to
not passing functions around as arguments, then it has exactly the
same expressive power as CFGs or SSA form.  FOL intraprocedural is
just that same form, but without those local procedure definitions.

I could match CFGs by introducing an expression for local (recursive)
function definitions with the restriction that such functions may
only be called in tail position.

This restriction would have the effect that anything that looks like a
return either flies into a statically known let binding, or flies out
of the entire FOL-procedure.

The thing that's unique about interprocedural stuff is that control
flows out of a procedure call into a place that's determined by the
place where it flowed in, and we care about tracing the correspondence
(whether it's implemented by watching the control flow explicitly or
by doing some data flow on some token (or continuation function) that
says where to go is immaterial).

The tail-calls-only restriction also means that you can only model
processes that take a bounded amount of modeled storage to execute (to
wit, the number of static variable bindings in the structure).  What I
mean by "modeled" is that the program is still free to accumulate
stuff either by hitting successive locations in the main memory or by
building itself a nice linked functional data structure; but
presumably analyzing what's going on in places that are not
temporaries is beyond the scope of the model (FOL or CFG).

By not providing such local definition facilities, FOL conflates
tail-call-only procedures with full procedures, and pushes them out of
the scope of FOL's intraprocedural analyses.

One of the consequences of this decision is that FOL cannot express loops
and loop optimizations.


FOL intraprocedural: Dataflows on an acyclic control flow graph.  This
implies a bounded execution time and therefore a finite number of
modeled memory bits.  These are easy to do by following a topological
sort of the control flow graph (FOL source happens to store these
graphs in topologically sorted form).  Break the full program into
pieces with acyclic control (FOL procedures) and don't model flows of
control or data across the introduced boundaries.

Appel intraprocedural: Dataflows with a finite number of modeled bits
of data but an arbitrary control flow graph.  Break the whole program
into pieces (procedures) that each only use a finite number of modeled
bits of memory (temporaries), and don't model flows of control or data
across the boundaries of these pieces.

Interprocedural: Dataflows with unbounded modeled locations (because
of the procedure stack) and an arbitrary control flow graph.  Since
this is Turing complete, you can't ever get this "right".  You have to
forget somewhere.


The latter but also tracking context: control will flow out of a
procedure call into a place that is determined by where it flowed into
that procedure call.
- Also, if control flowed from here, then I know this about the data
  that flowed in, which I don't know in general.
- I think you can go quite wild wrt the fineness of the context you
  track.


The art of analyzing programs is the art of forgetting judiciously.
If you forget nothing at all, your "analysis" will consist exactly of
running the entire program and writing down the answer.  In this case,
you will have "accelerated" the program a great deal, but the analysis
will be very slow (namely the runtime of the program) and its range of
validity will be that one run of that one program, so you will not be
able to reuse it.  If you forget everything, your analysis will
consist of doing nothing, will be very fast and very broadly
applicable, but will permit no improvement to the program you were
analyzing.

Different optimizing analyses are characterized by what they model and
what they forget, and different choices affect the speed of the
analysis, its reusability in different situations, and the amount of
improvement that doing it offers.  These three things are not traded
off immaculately; there are sweet spots in the design space that make
some sets of these choices definitely better than others for large and
predictable classes of programs.  In fact, the very reason we program
at all is that programs can cause computers to do "the same" things
many times, so that describing the task once to a computer is easier
than performing it that many times oneself.

A compiler analysis that forgets the differences between different
instances of such a task but remembers the characteristics that make
this task different from other tasks could execute in time
proportional to the complexity of the task, but yield total
performance improvements proportional to the number of times the task
is performed; if the latter multiple is enough larger than the former,
the analysis is worthwhile.

The purpose of this essay is to characterize the things the FOL
optimizers forget and remember, in contrast with the corresponding
choices made by other compiler organizations; and to discuss the
consequences these choices have on the structure of the FOL
implementation, the speed with which it executes, and the sorts of
optimizations it can and cannot perform.

Every ahead-of-time compiler commits, by its very nature, to at least
one form of forgetting: it must "forget", because it perforce does not
know, the data that the compilee program will operate on at runtime.
Anything that will be read off of disk files or network servers or
command terminals must necessarily be unknown to the compiler.  This
means that ahead-of-time compilation can at most be as slow as the
complexity (in whatever sense is appropriate) of the program being
compiled, must be applicable to any possible runtime data that the
program might be exposed to, and ought to yield optimization fruit if
the program experiences a sufficiently large total runtime.  Choices
made within the design space of ahead-of-time compilers are tradeoffs
within those bounds.

This enforced separation between what is already known and what is yet
to be known leads the mind naturally to the idea of the separation
between "program" and "data".  The "program" is the essentially fixed
thing we are thinking about how to improve, and the "data" is the
stuff that it will operate on in the future, to all possible values of
which we must be applicable.  There are other social and historical
forces that tend to make programs have one kind of structure and data
another; there are phenomena that blur those distinctions, for Lisp's
storied EVAL takes some "data" and causes it to become "program".
Indeed, if one is thinking about the compiler, the compiler itself is
the "program" and the compilee is the "data"; and if the compiler is
self-hosting, then the compiler can also play the role of "data".

Nevertheless, there is a natural separation between "program" and
"data".  The question of which individual atom of a program will be
executed when is called "control flow", and the question of what
journey some datum will take is called "data flow".  Even in the
absence of EVAL, in a higher-order program the control flow is
intertwined with the data flow, because a call site can invoke a
function that flows in through a variable.  In the beginning they
must, therefore, be analyzed together.  This is the job of VL/DVL, and
its peers like CFA (k- or otherwise) [Shivers].

FOL enters the scene after the integrated flow analysis.  The
essential scope-limiting assumption in FOL and its peers is that the
effect of the data flow on the control flow has been studied as far as
it will be, and FOL will study it no more.  Places of course remain
where the data must affect the control, (for example, IF nodes), but
by the time one reaches FOL it is considered good enough to assume
that an IF can go either way, and stop trying to predict which way it
will go.

Once this assumption is made, there remains some set of paths that one
deems it possible that the running program will take --- sequences of
atomic program elements that it may execute.  Trying to write down the
complete list thereof is both futile and brittle, so it must, of
necessity, be approximated.  A natural approximation is a static
Control Flow Graph (CFG) --- a directed graph whose nodes are atomic
program operations and whose edges are possible control transfers
between them.  The set of all the possible paths through such a graph
is one possible approximation to the set of execution paths that will
occur in a running program.

In programs in the wild, the actual control paths in fact have more
structure than a single, history-free CFG will represent.  There are
procedures, and call sites, and the procedures return to their callers
when done; whereas a CFG node for the end of a procedure must of
necessity list all the places where that procedure is called as
possible targets of outward transfers.  In practice, which way control
will go on exit depends on how it entered, but a direct CFG cannot
capture this.  There are several ways to approach this difficulty: one
can introduce additional data tokens that say where to transfer
control next, in which case one is back to data flow affecting control
flow; one can replicate subgraphs of the control flow graph depending
on context, in which case one must somehow decide when to stop, lest
one's CFG grow beyond all bound; one can somehow represent more than
just the graph structure, with the same caveat; or one can just forget
about it, and get coarser analyses.  The choice of whether or not to
model this behavior distinguishes intraprocedural analyses from
interprocedural ones.

If one assumes that one will not model the relationship between where
a procedure was called from and where it will return, the finest-grain
representation for modeling what's left is equivalent to the control
flow graphs taught by Appel [1].  That same book also discusses Static
Single Assignment Form, which is just a normalized form for a control
flow graph, so I will not discuss the distinction.


Just as one has choices to make about what aspects of control flow one
will model, one also has choices about what aspects of data flow one
will model.






- Memory model

- One must still cut the program (which is how one can lose context
  and let a function that is constant somewhere else become unknown
  here even in the absence of EVAL)
